{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-GPT text prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-06-26 20:26:25--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8003::154, 2606:50c0:8001::154, 2606:50c0:8002::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8003::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt.2’\n",
      "\n",
      "input.txt.2         100%[===================>]   1.06M  --.-KB/s    in 0.1s    \n",
      "\n",
      "2024-06-26 20:26:25 (11.2 MB/s) - ‘input.txt.2’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import dataset\n",
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'mps'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset (characters): 1115394\n",
      "first 100 characters: First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us\n"
     ]
    }
   ],
   "source": [
    "with open('input.txt', 'r', encoding = 'utf-8') as file:\n",
    "    data_str = file.read()\n",
    "print(f'length of dataset (characters): {len(data_str)}')\n",
    "print(f'first 100 characters: {data_str[:300]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1115394 characters, 65 unique.\n",
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "# Establish 'vocabulary' of characters\n",
    "chars = sorted(list(set(data_str)))\n",
    "data_size, vocab_size = len(data_str), len(chars)\n",
    "print(f'data has {data_size} characters, {vocab_size} unique.')\n",
    "print(''.join(chars))\n",
    "\n",
    "# Create dictionaries to convert characters to integers and vice versa\n",
    "char_to_ix = {ch:i for i,ch in enumerate(chars)}\n",
    "ix_to_char = {i:ch for i,ch in enumerate(chars)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: [46], Decoded: h\n"
     ]
    }
   ],
   "source": [
    "# Encoder takes a string and retuns a list of integers\n",
    "encode = lambda string: [char_to_ix[ch] for ch in string]\n",
    "decode = lambda lst: ''.join([ix_to_char[int(ix)] for ix in lst])\n",
    "\n",
    "usript = input('Enter text to encode: ')\n",
    "print(f'Encoded: {encode(usript)}, Decoded: {decode(encode(usript))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(data_str), dtype=torch.long, device = device)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "block_size = 32\n",
    "lr = 1e-3\n",
    "steps = 5000\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: tensor([18], device='mps:0'), Target: 47\n",
      "Decoded: F -> i\n",
      "Context: tensor([18, 47], device='mps:0'), Target: 56\n",
      "Decoded: Fi -> r\n",
      "Context: tensor([18, 47, 56], device='mps:0'), Target: 57\n",
      "Decoded: Fir -> s\n",
      "Context: tensor([18, 47, 56, 57], device='mps:0'), Target: 58\n",
      "Decoded: Firs -> t\n",
      "Context: tensor([18, 47, 56, 57, 58], device='mps:0'), Target: 1\n",
      "Decoded: First ->  \n",
      "Context: tensor([18, 47, 56, 57, 58,  1], device='mps:0'), Target: 15\n",
      "Decoded: First  -> C\n",
      "Context: tensor([18, 47, 56, 57, 58,  1, 15], device='mps:0'), Target: 47\n",
      "Decoded: First C -> i\n",
      "Context: tensor([18, 47, 56, 57, 58,  1, 15, 47], device='mps:0'), Target: 58\n",
      "Decoded: First Ci -> t\n",
      "Context: tensor([18, 47, 56, 57, 58,  1, 15, 47, 58], device='mps:0'), Target: 47\n",
      "Decoded: First Cit -> i\n",
      "Context: tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47], device='mps:0'), Target: 64\n",
      "Decoded: First Citi -> z\n",
      "Context: tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64], device='mps:0'), Target: 43\n",
      "Decoded: First Citiz -> e\n",
      "Context: tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43], device='mps:0'), Target: 52\n",
      "Decoded: First Citize -> n\n",
      "Context: tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52], device='mps:0'), Target: 10\n",
      "Decoded: First Citizen -> :\n",
      "Context: tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10],\n",
      "       device='mps:0'), Target: 0\n",
      "Decoded: First Citizen: -> \n",
      "\n",
      "Context: tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0],\n",
      "       device='mps:0'), Target: 14\n",
      "Decoded: First Citizen:\n",
      " -> B\n",
      "Context: tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14],\n",
      "       device='mps:0'), Target: 43\n",
      "Decoded: First Citizen:\n",
      "B -> e\n",
      "Context: tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43],\n",
      "       device='mps:0'), Target: 44\n",
      "Decoded: First Citizen:\n",
      "Be -> f\n",
      "Context: tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44],\n",
      "       device='mps:0'), Target: 53\n",
      "Decoded: First Citizen:\n",
      "Bef -> o\n",
      "Context: tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53], device='mps:0'), Target: 56\n",
      "Decoded: First Citizen:\n",
      "Befo -> r\n",
      "Context: tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56], device='mps:0'), Target: 43\n",
      "Decoded: First Citizen:\n",
      "Befor -> e\n",
      "Context: tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43], device='mps:0'), Target: 1\n",
      "Decoded: First Citizen:\n",
      "Before ->  \n",
      "Context: tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1], device='mps:0'), Target: 61\n",
      "Decoded: First Citizen:\n",
      "Before  -> w\n",
      "Context: tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61], device='mps:0'), Target: 43\n",
      "Decoded: First Citizen:\n",
      "Before w -> e\n",
      "Context: tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43], device='mps:0'), Target: 1\n",
      "Decoded: First Citizen:\n",
      "Before we ->  \n",
      "Context: tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1], device='mps:0'), Target: 54\n",
      "Decoded: First Citizen:\n",
      "Before we  -> p\n",
      "Context: tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54], device='mps:0'), Target: 56\n",
      "Decoded: First Citizen:\n",
      "Before we p -> r\n",
      "Context: tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56], device='mps:0'), Target: 53\n",
      "Decoded: First Citizen:\n",
      "Before we pr -> o\n",
      "Context: tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53], device='mps:0'), Target: 41\n",
      "Decoded: First Citizen:\n",
      "Before we pro -> c\n",
      "Context: tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41], device='mps:0'), Target: 43\n",
      "Decoded: First Citizen:\n",
      "Before we proc -> e\n",
      "Context: tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43], device='mps:0'), Target: 43\n",
      "Decoded: First Citizen:\n",
      "Before we proce -> e\n",
      "Context: tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43], device='mps:0'), Target: 42\n",
      "Decoded: First Citizen:\n",
      "Before we procee -> d\n",
      "Context: tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42],\n",
      "       device='mps:0'), Target: 1\n",
      "Decoded: First Citizen:\n",
      "Before we proceed ->  \n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f'Context: {context}, Target: {target}')\n",
    "    print(f'Decoded: {decode(context)} -> {decode([target])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention mechanism\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\"One-headed self attention mechanism\"\"\"\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias = False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias = False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias = False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        K = self.key(x) # (B,T,C)\n",
    "        Q = self.query(x) # (B,T,C)\n",
    "        # Compute attention scores\n",
    "        wei = Q@K.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T,:T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        # Apply attention scores to values\n",
    "        V = self.value(x)\n",
    "        att = wei @ V\n",
    "        return att\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "    \n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.209729 parameters\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1443)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Each token reads off the logits for the next token\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "model.to(device)\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'parameters')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([16, 32])\n",
      "tensor([[ 0, 28, 56, 53, 41, 50, 39, 47, 51,  1, 47, 58,  6,  1, 54, 56, 53, 60,\n",
      "         53, 57, 58,  6,  1, 56, 53, 59, 52, 42,  1, 39, 40, 53],\n",
      "        [10,  1, 47, 44,  1, 58, 46, 39, 58,  1, 41, 53, 51, 43,  1, 57, 46, 53,\n",
      "         56, 58,  6,  0, 27, 59, 56,  1, 57, 59, 40, 57, 58, 47],\n",
      "        [ 1, 46, 39, 60, 43,  1, 42, 47, 57, 54, 39, 58, 41, 46,  5, 42,  0, 35,\n",
      "         47, 58, 46,  1, 50, 43, 58, 58, 43, 56, 57,  1, 53, 44],\n",
      "        [46, 43, 57, 43,  1, 43, 63, 43, 57,  1, 53, 44,  1, 51, 47, 52, 43,  0,\n",
      "         27, 56,  1, 44, 53, 56, 58, 59, 52, 43,  1, 45, 47, 60],\n",
      "        [58, 47, 53, 52, 11,  1, 63, 53, 59,  1, 57, 46, 39, 50, 50,  0, 54, 43,\n",
      "         56, 41, 43, 47, 60, 43,  1, 58, 46, 39, 58,  1, 39,  1],\n",
      "        [56,  6,  0, 15, 59, 58,  1, 53, 44, 44,  1, 58, 46, 43,  1, 46, 43, 39,\n",
      "         42, 57,  1, 53, 44,  1, 58, 53, 53,  1, 44, 39, 57, 58],\n",
      "        [ 6,  1, 47, 52,  1, 37, 53, 56, 49, 57, 46, 47, 56, 43,  1, 39, 56, 43,\n",
      "          1, 47, 52,  1, 39, 56, 51, 57,  8,  0, 37, 43, 58,  1],\n",
      "        [35, 43,  5, 50, 50,  1, 46, 39, 60, 43,  1, 58, 46, 47, 57,  1, 57, 53,\n",
      "         52, 45,  1, 53, 59, 58,  1, 39, 52, 53, 52,  1, 40, 63],\n",
      "        [47, 57,  1, 40, 43, 12,  7,  7,  0, 35, 47, 58, 46,  1, 61, 46, 39, 58,\n",
      "          5, 57,  1, 59, 52, 56, 43, 39, 50,  1, 58, 46, 53, 59],\n",
      "        [43,  6,  1, 25, 39, 56, 45, 39, 56, 43, 58, 11,  1, 54, 39, 56, 42, 53,\n",
      "         52,  1, 51, 43,  6,  1, 57, 61, 43, 43, 58,  1, 57, 53],\n",
      "        [39, 56, 43,  1, 63, 53, 59,  1, 61, 43, 50, 50,  6,  1, 58, 46, 43, 52,\n",
      "          8,  0, 15, 53, 51, 43,  6,  1, 45, 53, 53, 42,  1, 57],\n",
      "        [46, 39, 58,  1, 57, 50, 39, 52, 42, 43, 56, 57,  1, 46, 47, 51,  1, 61,\n",
      "         47, 58, 46,  1, 41, 53, 61, 39, 56, 42, 47, 41, 43,  0],\n",
      "        [46, 43, 56,  1, 39, 58,  1, 58, 46, 43,  1, 60, 47, 43, 61, 11,  0, 13,\n",
      "         52, 42,  1, 58, 46, 39, 58,  1, 40, 43,  1, 46, 43, 47],\n",
      "        [43,  1, 46, 53, 52, 53, 59, 56,  5, 42,  1, 52, 59, 51, 40, 43, 56,  6,\n",
      "          0, 35, 46, 53,  1, 50, 39, 41, 49,  1, 52, 53, 58,  1],\n",
      "        [58, 47, 50, 50,  8,  0, 15, 53, 59, 56, 39, 45, 43,  2,  1, 58, 46, 43,\n",
      "         56, 43,  1, 61, 47, 50, 50,  1, 40, 43,  1, 54, 47, 58],\n",
      "        [53, 59,  5, 50, 50,  1, 52, 53, 58,  1, 43, 52, 42, 59, 56, 43,  1, 46,\n",
      "         47, 51,  2,  1, 19, 53, 42,  1, 57, 46, 39, 50, 50,  1]],\n",
      "       device='mps:0')\n",
      "targets:\n",
      "torch.Size([16, 32])\n",
      "tensor([[28, 56, 53, 41, 50, 39, 47, 51,  1, 47, 58,  6,  1, 54, 56, 53, 60, 53,\n",
      "         57, 58,  6,  1, 56, 53, 59, 52, 42,  1, 39, 40, 53, 59],\n",
      "        [ 1, 47, 44,  1, 58, 46, 39, 58,  1, 41, 53, 51, 43,  1, 57, 46, 53, 56,\n",
      "         58,  6,  0, 27, 59, 56,  1, 57, 59, 40, 57, 58, 47, 58],\n",
      "        [46, 39, 60, 43,  1, 42, 47, 57, 54, 39, 58, 41, 46,  5, 42,  0, 35, 47,\n",
      "         58, 46,  1, 50, 43, 58, 58, 43, 56, 57,  1, 53, 44,  1],\n",
      "        [43, 57, 43,  1, 43, 63, 43, 57,  1, 53, 44,  1, 51, 47, 52, 43,  0, 27,\n",
      "         56,  1, 44, 53, 56, 58, 59, 52, 43,  1, 45, 47, 60, 43],\n",
      "        [47, 53, 52, 11,  1, 63, 53, 59,  1, 57, 46, 39, 50, 50,  0, 54, 43, 56,\n",
      "         41, 43, 47, 60, 43,  1, 58, 46, 39, 58,  1, 39,  1, 22],\n",
      "        [ 6,  0, 15, 59, 58,  1, 53, 44, 44,  1, 58, 46, 43,  1, 46, 43, 39, 42,\n",
      "         57,  1, 53, 44,  1, 58, 53, 53,  1, 44, 39, 57, 58,  1],\n",
      "        [ 1, 47, 52,  1, 37, 53, 56, 49, 57, 46, 47, 56, 43,  1, 39, 56, 43,  1,\n",
      "         47, 52,  1, 39, 56, 51, 57,  8,  0, 37, 43, 58,  1, 58],\n",
      "        [43,  5, 50, 50,  1, 46, 39, 60, 43,  1, 58, 46, 47, 57,  1, 57, 53, 52,\n",
      "         45,  1, 53, 59, 58,  1, 39, 52, 53, 52,  1, 40, 63,  1],\n",
      "        [57,  1, 40, 43, 12,  7,  7,  0, 35, 47, 58, 46,  1, 61, 46, 39, 58,  5,\n",
      "         57,  1, 59, 52, 56, 43, 39, 50,  1, 58, 46, 53, 59,  1],\n",
      "        [ 6,  1, 25, 39, 56, 45, 39, 56, 43, 58, 11,  1, 54, 39, 56, 42, 53, 52,\n",
      "          1, 51, 43,  6,  1, 57, 61, 43, 43, 58,  1, 57, 53, 52],\n",
      "        [56, 43,  1, 63, 53, 59,  1, 61, 43, 50, 50,  6,  1, 58, 46, 43, 52,  8,\n",
      "          0, 15, 53, 51, 43,  6,  1, 45, 53, 53, 42,  1, 57, 61],\n",
      "        [39, 58,  1, 57, 50, 39, 52, 42, 43, 56, 57,  1, 46, 47, 51,  1, 61, 47,\n",
      "         58, 46,  1, 41, 53, 61, 39, 56, 42, 47, 41, 43,  0, 35],\n",
      "        [43, 56,  1, 39, 58,  1, 58, 46, 43,  1, 60, 47, 43, 61, 11,  0, 13, 52,\n",
      "         42,  1, 58, 46, 39, 58,  1, 40, 43,  1, 46, 43, 47, 56],\n",
      "        [ 1, 46, 53, 52, 53, 59, 56,  5, 42,  1, 52, 59, 51, 40, 43, 56,  6,  0,\n",
      "         35, 46, 53,  1, 50, 39, 41, 49,  1, 52, 53, 58,  1, 60],\n",
      "        [47, 50, 50,  8,  0, 15, 53, 59, 56, 39, 45, 43,  2,  1, 58, 46, 43, 56,\n",
      "         43,  1, 61, 47, 50, 50,  1, 40, 43,  1, 54, 47, 58, 63],\n",
      "        [59,  5, 50, 50,  1, 52, 53, 58,  1, 43, 52, 42, 59, 56, 43,  1, 46, 47,\n",
      "         51,  2,  1, 19, 53, 42,  1, 57, 46, 39, 50, 50,  1, 51]],\n",
      "       device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1443)\n",
    "\n",
    "# batch_size = 32\n",
    "# block_size = 8\n",
    "# lr = 1e-3\n",
    "# steps = 1000\n",
    "# n_embd = 32\n",
    "# n_head = 4\n",
    "# n_layer = 4\n",
    "# dropout = 0.0\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(0, data.size(0) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+1+block_size] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "x_batch, y_batch = get_batch('train')\n",
    "print('inputs:')\n",
    "print(x_batch.shape)\n",
    "print(x_batch)\n",
    "print('targets:')\n",
    "print(y_batch.shape)\n",
    "print(y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.209729 parameters\n",
      "step 10, loss: 3.60117244720459\n",
      "step 20, loss: 3.4669625759124756\n",
      "step 30, loss: 3.0840859413146973\n",
      "step 40, loss: 3.0595035552978516\n",
      "step 50, loss: 2.9615254402160645\n",
      "step 60, loss: 2.873871088027954\n",
      "step 70, loss: 2.769955635070801\n",
      "step 80, loss: 2.7063753604888916\n",
      "step 90, loss: 2.7173914909362793\n",
      "step 100, loss: 2.5884854793548584\n",
      "step 110, loss: 2.6046488285064697\n",
      "step 120, loss: 2.610779285430908\n",
      "step 130, loss: 2.587836503982544\n",
      "step 140, loss: 2.6302900314331055\n",
      "step 150, loss: 2.598050117492676\n",
      "step 160, loss: 2.4461402893066406\n",
      "step 170, loss: 2.4879050254821777\n",
      "step 180, loss: 2.543548107147217\n",
      "step 190, loss: 2.4333014488220215\n",
      "step 200, loss: 2.553431272506714\n",
      "step 210, loss: 2.5812222957611084\n",
      "step 220, loss: 2.5023536682128906\n",
      "step 230, loss: 2.541884183883667\n",
      "step 240, loss: 2.3150887489318848\n",
      "step 250, loss: 2.435875415802002\n",
      "step 260, loss: 2.3891899585723877\n",
      "step 270, loss: 2.4788079261779785\n",
      "step 280, loss: 2.4960851669311523\n",
      "step 290, loss: 2.435171127319336\n",
      "step 300, loss: 2.4081780910491943\n",
      "step 310, loss: 2.4879655838012695\n",
      "step 320, loss: 2.4272239208221436\n",
      "step 330, loss: 2.2631783485412598\n",
      "step 340, loss: 2.45389461517334\n",
      "step 350, loss: 2.450434684753418\n",
      "step 360, loss: 2.28962779045105\n",
      "step 370, loss: 2.469601631164551\n",
      "step 380, loss: 2.4261605739593506\n",
      "step 390, loss: 2.3652939796447754\n",
      "step 400, loss: 2.363102436065674\n",
      "step 410, loss: 2.38785457611084\n",
      "step 420, loss: 2.4419069290161133\n",
      "step 430, loss: 2.3687310218811035\n",
      "step 440, loss: 2.2732748985290527\n",
      "step 450, loss: 2.3968114852905273\n",
      "step 460, loss: 2.3488216400146484\n",
      "step 470, loss: 2.3564493656158447\n",
      "step 480, loss: 2.472463369369507\n",
      "step 490, loss: 2.371216297149658\n",
      "step 500, loss: 2.2394280433654785\n",
      "step 510, loss: 2.3203091621398926\n",
      "step 520, loss: 2.3206188678741455\n",
      "step 530, loss: 2.2558186054229736\n",
      "step 540, loss: 2.2259552478790283\n",
      "step 550, loss: 2.267594337463379\n",
      "step 560, loss: 2.2870097160339355\n",
      "step 570, loss: 2.2154603004455566\n",
      "step 580, loss: 2.3539984226226807\n",
      "step 590, loss: 2.241727352142334\n",
      "step 600, loss: 2.3093106746673584\n",
      "step 610, loss: 2.289973258972168\n",
      "step 620, loss: 2.2272472381591797\n",
      "step 630, loss: 2.3438162803649902\n",
      "step 640, loss: 2.115541458129883\n",
      "step 650, loss: 2.1453986167907715\n",
      "step 660, loss: 2.220994472503662\n",
      "step 670, loss: 2.239480972290039\n",
      "step 680, loss: 2.1737060546875\n",
      "step 690, loss: 2.2300071716308594\n",
      "step 700, loss: 2.099008560180664\n",
      "step 710, loss: 2.187551736831665\n",
      "step 720, loss: 2.1292965412139893\n",
      "step 730, loss: 2.334648370742798\n",
      "step 740, loss: 2.292792320251465\n",
      "step 750, loss: 2.1436355113983154\n",
      "step 760, loss: 2.285449981689453\n",
      "step 770, loss: 2.1865193843841553\n",
      "step 780, loss: 2.2272214889526367\n",
      "step 790, loss: 2.1340808868408203\n",
      "step 800, loss: 2.1422200202941895\n",
      "step 810, loss: 2.222240924835205\n",
      "step 820, loss: 2.276397705078125\n",
      "step 830, loss: 2.1687450408935547\n",
      "step 840, loss: 2.106538772583008\n",
      "step 850, loss: 2.173576593399048\n",
      "step 860, loss: 2.1322684288024902\n",
      "step 870, loss: 2.2041819095611572\n",
      "step 880, loss: 2.1456971168518066\n",
      "step 890, loss: 2.1479434967041016\n",
      "step 900, loss: 2.106363296508789\n",
      "step 910, loss: 2.1816558837890625\n",
      "step 920, loss: 2.089588165283203\n",
      "step 930, loss: 2.15519380569458\n",
      "step 940, loss: 2.180685520172119\n",
      "step 950, loss: 2.052719831466675\n",
      "step 960, loss: 2.1003525257110596\n",
      "step 970, loss: 2.152597427368164\n",
      "step 980, loss: 2.1358139514923096\n",
      "step 990, loss: 2.039196491241455\n",
      "step 1000, loss: 2.045907974243164\n",
      "step 1010, loss: 2.109938621520996\n",
      "step 1020, loss: 2.0829389095306396\n",
      "step 1030, loss: 2.033301830291748\n",
      "step 1040, loss: 2.1954758167266846\n",
      "step 1050, loss: 2.041408061981201\n",
      "step 1060, loss: 2.109543561935425\n",
      "step 1070, loss: 2.1553568840026855\n",
      "step 1080, loss: 2.154754161834717\n",
      "step 1090, loss: 2.1470837593078613\n",
      "step 1100, loss: 2.122488021850586\n",
      "step 1110, loss: 2.031090021133423\n",
      "step 1120, loss: 2.0199215412139893\n",
      "step 1130, loss: 2.0671310424804688\n",
      "step 1140, loss: 2.046992301940918\n",
      "step 1150, loss: 2.0877268314361572\n",
      "step 1160, loss: 2.0058131217956543\n",
      "step 1170, loss: 2.027055025100708\n",
      "step 1180, loss: 1.9111379384994507\n",
      "step 1190, loss: 1.986844778060913\n",
      "step 1200, loss: 1.9832124710083008\n",
      "step 1210, loss: 1.9797000885009766\n",
      "step 1220, loss: 2.0528643131256104\n",
      "step 1230, loss: 2.1248779296875\n",
      "step 1240, loss: 2.0106992721557617\n",
      "step 1250, loss: 1.9768903255462646\n",
      "step 1260, loss: 2.0473837852478027\n",
      "step 1270, loss: 2.050774097442627\n",
      "step 1280, loss: 2.038912773132324\n",
      "step 1290, loss: 2.0242414474487305\n",
      "step 1300, loss: 1.9490571022033691\n",
      "step 1310, loss: 1.9753118753433228\n",
      "step 1320, loss: 2.0200092792510986\n",
      "step 1330, loss: 1.8670101165771484\n",
      "step 1340, loss: 1.887291669845581\n",
      "step 1350, loss: 2.0174131393432617\n",
      "step 1360, loss: 2.0907881259918213\n",
      "step 1370, loss: 1.9331611394882202\n",
      "step 1380, loss: 2.0372910499572754\n",
      "step 1390, loss: 1.9411276578903198\n",
      "step 1400, loss: 1.931191086769104\n",
      "step 1410, loss: 1.9082655906677246\n",
      "step 1420, loss: 1.9240785837173462\n",
      "step 1430, loss: 2.0378825664520264\n",
      "step 1440, loss: 2.0994253158569336\n",
      "step 1450, loss: 1.9484424591064453\n",
      "step 1460, loss: 2.028991460800171\n",
      "step 1470, loss: 2.168259620666504\n",
      "step 1480, loss: 1.976945161819458\n",
      "step 1490, loss: 1.8905055522918701\n",
      "step 1500, loss: 2.0066933631896973\n",
      "step 1510, loss: 1.9922394752502441\n",
      "step 1520, loss: 1.871128797531128\n",
      "step 1530, loss: 2.0377182960510254\n",
      "step 1540, loss: 1.9315845966339111\n",
      "step 1550, loss: 1.9370461702346802\n",
      "step 1560, loss: 1.9790589809417725\n",
      "step 1570, loss: 1.9521251916885376\n",
      "step 1580, loss: 1.928483009338379\n",
      "step 1590, loss: 2.0497639179229736\n",
      "step 1600, loss: 1.8069567680358887\n",
      "step 1610, loss: 2.0156240463256836\n",
      "step 1620, loss: 1.9741976261138916\n",
      "step 1630, loss: 1.9145039319992065\n",
      "step 1640, loss: 1.9604028463363647\n",
      "step 1650, loss: 1.9283777475357056\n",
      "step 1660, loss: 1.8949270248413086\n",
      "step 1670, loss: 1.80718994140625\n",
      "step 1680, loss: 1.9224004745483398\n",
      "step 1690, loss: 1.9406147003173828\n",
      "step 1700, loss: 1.9702749252319336\n",
      "step 1710, loss: 2.0421276092529297\n",
      "step 1720, loss: 1.912559986114502\n",
      "step 1730, loss: 1.973768949508667\n",
      "step 1740, loss: 1.9633467197418213\n",
      "step 1750, loss: 1.8757319450378418\n",
      "step 1760, loss: 1.8011062145233154\n",
      "step 1770, loss: 2.0228111743927\n",
      "step 1780, loss: 2.145217180252075\n",
      "step 1790, loss: 2.0292515754699707\n",
      "step 1800, loss: 1.9080500602722168\n",
      "step 1810, loss: 2.002147674560547\n",
      "step 1820, loss: 1.803542971611023\n",
      "step 1830, loss: 1.8928921222686768\n",
      "step 1840, loss: 1.9097681045532227\n",
      "step 1850, loss: 1.888329267501831\n",
      "step 1860, loss: 1.8708250522613525\n",
      "step 1870, loss: 1.9170242547988892\n",
      "step 1880, loss: 1.831547498703003\n",
      "step 1890, loss: 1.7946888208389282\n",
      "step 1900, loss: 1.9796202182769775\n",
      "step 1910, loss: 1.8408586978912354\n",
      "step 1920, loss: 1.7814018726348877\n",
      "step 1930, loss: 1.961421251296997\n",
      "step 1940, loss: 1.8775255680084229\n",
      "step 1950, loss: 1.8996187448501587\n",
      "step 1960, loss: 1.8123308420181274\n",
      "step 1970, loss: 1.9419198036193848\n",
      "step 1980, loss: 1.8902219533920288\n",
      "step 1990, loss: 1.8246742486953735\n",
      "step 2000, loss: 1.7623140811920166\n",
      "step 2010, loss: 1.8002574443817139\n",
      "step 2020, loss: 1.8256856203079224\n",
      "step 2030, loss: 1.7356195449829102\n",
      "step 2040, loss: 1.6928040981292725\n",
      "step 2050, loss: 1.9748938083648682\n",
      "step 2060, loss: 1.9577115774154663\n",
      "step 2070, loss: 1.817063331604004\n",
      "step 2080, loss: 1.9579966068267822\n",
      "step 2090, loss: 1.947150468826294\n",
      "step 2100, loss: 1.8536909818649292\n",
      "step 2110, loss: 1.9050486087799072\n",
      "step 2120, loss: 1.873762845993042\n",
      "step 2130, loss: 1.933401107788086\n",
      "step 2140, loss: 1.8925259113311768\n",
      "step 2150, loss: 1.9410171508789062\n",
      "step 2160, loss: 1.924270510673523\n",
      "step 2170, loss: 1.8419365882873535\n",
      "step 2180, loss: 1.821272373199463\n",
      "step 2190, loss: 1.909924030303955\n",
      "step 2200, loss: 1.9107797145843506\n",
      "step 2210, loss: 1.9610247611999512\n",
      "step 2220, loss: 1.8386003971099854\n",
      "step 2230, loss: 1.9046189785003662\n",
      "step 2240, loss: 1.8537055253982544\n",
      "step 2250, loss: 1.8377561569213867\n",
      "step 2260, loss: 1.7297238111495972\n",
      "step 2270, loss: 1.9910866022109985\n",
      "step 2280, loss: 1.8611350059509277\n",
      "step 2290, loss: 1.8907225131988525\n",
      "step 2300, loss: 1.9599841833114624\n",
      "step 2310, loss: 1.8816778659820557\n",
      "step 2320, loss: 1.856188416481018\n",
      "step 2330, loss: 1.8596681356430054\n",
      "step 2340, loss: 1.920306921005249\n",
      "step 2350, loss: 1.9753772020339966\n",
      "step 2360, loss: 1.9266395568847656\n",
      "step 2370, loss: 1.8150756359100342\n",
      "step 2380, loss: 1.8545615673065186\n",
      "step 2390, loss: 1.9102401733398438\n",
      "step 2400, loss: 1.8778297901153564\n",
      "step 2410, loss: 1.9278773069381714\n",
      "step 2420, loss: 1.7833003997802734\n",
      "step 2430, loss: 1.9070082902908325\n",
      "step 2440, loss: 1.8236759901046753\n",
      "step 2450, loss: 1.7942802906036377\n",
      "step 2460, loss: 1.8064672946929932\n",
      "step 2470, loss: 1.9077292680740356\n",
      "step 2480, loss: 1.8008486032485962\n",
      "step 2490, loss: 1.8529760837554932\n",
      "step 2500, loss: 1.7778769731521606\n",
      "step 2510, loss: 1.7910999059677124\n",
      "step 2520, loss: 1.7019810676574707\n",
      "step 2530, loss: 1.7234376668930054\n",
      "step 2540, loss: 1.7465864419937134\n",
      "step 2550, loss: 1.8501262664794922\n",
      "step 2560, loss: 1.8686453104019165\n",
      "step 2570, loss: 1.6753079891204834\n",
      "step 2580, loss: 1.7721879482269287\n",
      "step 2590, loss: 1.9422439336776733\n",
      "step 2600, loss: 1.7228021621704102\n",
      "step 2610, loss: 1.9755218029022217\n",
      "step 2620, loss: 1.8230829238891602\n",
      "step 2630, loss: 1.6900405883789062\n",
      "step 2640, loss: 1.72438645362854\n",
      "step 2650, loss: 1.8072991371154785\n",
      "step 2660, loss: 1.7555019855499268\n",
      "step 2670, loss: 1.8822791576385498\n",
      "step 2680, loss: 1.836930274963379\n",
      "step 2690, loss: 1.8468360900878906\n",
      "step 2700, loss: 1.7829644680023193\n",
      "step 2710, loss: 1.7313365936279297\n",
      "step 2720, loss: 1.7366150617599487\n",
      "step 2730, loss: 1.874007225036621\n",
      "step 2740, loss: 1.750779390335083\n",
      "step 2750, loss: 1.7452597618103027\n",
      "step 2760, loss: 1.7101136445999146\n",
      "step 2770, loss: 1.9632117748260498\n",
      "step 2780, loss: 1.6565663814544678\n",
      "step 2790, loss: 1.78812837600708\n",
      "step 2800, loss: 1.768778920173645\n",
      "step 2810, loss: 1.6255168914794922\n",
      "step 2820, loss: 1.774917721748352\n",
      "step 2830, loss: 1.9089250564575195\n",
      "step 2840, loss: 1.9422388076782227\n",
      "step 2850, loss: 1.8086501359939575\n",
      "step 2860, loss: 1.8729465007781982\n",
      "step 2870, loss: 1.8833870887756348\n",
      "step 2880, loss: 1.9098732471466064\n",
      "step 2890, loss: 1.9530916213989258\n",
      "step 2900, loss: 1.7581971883773804\n",
      "step 2910, loss: 1.8205292224884033\n",
      "step 2920, loss: 1.7311785221099854\n",
      "step 2930, loss: 1.7169456481933594\n",
      "step 2940, loss: 1.7674977779388428\n",
      "step 2950, loss: 1.704082727432251\n",
      "step 2960, loss: 1.8370879888534546\n",
      "step 2970, loss: 1.800600528717041\n",
      "step 2980, loss: 1.6690093278884888\n",
      "step 2990, loss: 1.8108656406402588\n",
      "step 3000, loss: 1.780436635017395\n",
      "step 3010, loss: 1.759594202041626\n",
      "step 3020, loss: 1.719614028930664\n",
      "step 3030, loss: 1.858080267906189\n",
      "step 3040, loss: 1.7144711017608643\n",
      "step 3050, loss: 1.8369903564453125\n",
      "step 3060, loss: 1.749366283416748\n",
      "step 3070, loss: 1.8419697284698486\n",
      "step 3080, loss: 1.6569515466690063\n",
      "step 3090, loss: 1.7154525518417358\n",
      "step 3100, loss: 1.6394097805023193\n",
      "step 3110, loss: 1.8238906860351562\n",
      "step 3120, loss: 1.7163574695587158\n",
      "step 3130, loss: 1.820404052734375\n",
      "step 3140, loss: 1.7723474502563477\n",
      "step 3150, loss: 1.7757622003555298\n",
      "step 3160, loss: 1.720278024673462\n",
      "step 3170, loss: 1.725412130355835\n",
      "step 3180, loss: 1.6559044122695923\n",
      "step 3190, loss: 1.6952238082885742\n",
      "step 3200, loss: 1.7256815433502197\n",
      "step 3210, loss: 1.8576545715332031\n",
      "step 3220, loss: 1.7956788539886475\n",
      "step 3230, loss: 1.7142202854156494\n",
      "step 3240, loss: 1.6890108585357666\n",
      "step 3250, loss: 1.7381043434143066\n",
      "step 3260, loss: 1.5770084857940674\n",
      "step 3270, loss: 1.808631181716919\n",
      "step 3280, loss: 1.697317123413086\n",
      "step 3290, loss: 1.7516262531280518\n",
      "step 3300, loss: 1.6907050609588623\n",
      "step 3310, loss: 1.7637522220611572\n",
      "step 3320, loss: 1.7748711109161377\n",
      "step 3330, loss: 1.6510024070739746\n",
      "step 3340, loss: 1.7992963790893555\n",
      "step 3350, loss: 1.860511302947998\n",
      "step 3360, loss: 1.7253388166427612\n",
      "step 3370, loss: 1.5303585529327393\n",
      "step 3380, loss: 1.746525764465332\n",
      "step 3390, loss: 1.6143522262573242\n",
      "step 3400, loss: 1.7886712551116943\n",
      "step 3410, loss: 1.605666160583496\n",
      "step 3420, loss: 1.8975669145584106\n",
      "step 3430, loss: 1.7057849168777466\n",
      "step 3440, loss: 1.7054072618484497\n",
      "step 3450, loss: 1.7756105661392212\n",
      "step 3460, loss: 1.8410894870758057\n",
      "step 3470, loss: 1.7606797218322754\n",
      "step 3480, loss: 1.811604380607605\n",
      "step 3490, loss: 1.6517953872680664\n",
      "step 3500, loss: 1.743361234664917\n",
      "step 3510, loss: 1.6398727893829346\n",
      "step 3520, loss: 1.6644513607025146\n",
      "step 3530, loss: 1.6624317169189453\n",
      "step 3540, loss: 1.615133285522461\n",
      "step 3550, loss: 1.703575611114502\n",
      "step 3560, loss: 1.678596019744873\n",
      "step 3570, loss: 1.7635645866394043\n",
      "step 3580, loss: 1.7146259546279907\n",
      "step 3590, loss: 1.758232831954956\n",
      "step 3600, loss: 1.745567798614502\n",
      "step 3610, loss: 1.7041970491409302\n",
      "step 3620, loss: 1.726881980895996\n",
      "step 3630, loss: 1.687648057937622\n",
      "step 3640, loss: 1.6774948835372925\n",
      "step 3650, loss: 1.7098585367202759\n",
      "step 3660, loss: 1.7878942489624023\n",
      "step 3670, loss: 1.6918785572052002\n",
      "step 3680, loss: 1.6237907409667969\n",
      "step 3690, loss: 1.7882860898971558\n",
      "step 3700, loss: 1.7261935472488403\n",
      "step 3710, loss: 1.6817212104797363\n",
      "step 3720, loss: 1.706392765045166\n",
      "step 3730, loss: 1.6972626447677612\n",
      "step 3740, loss: 1.8060815334320068\n",
      "step 3750, loss: 1.690588355064392\n",
      "step 3760, loss: 1.6738166809082031\n",
      "step 3770, loss: 1.75754976272583\n",
      "step 3780, loss: 1.5944437980651855\n",
      "step 3790, loss: 1.7906430959701538\n",
      "step 3800, loss: 1.6276311874389648\n",
      "step 3810, loss: 1.661958932876587\n",
      "step 3820, loss: 1.6597247123718262\n",
      "step 3830, loss: 1.7557823657989502\n",
      "step 3840, loss: 1.7320953607559204\n",
      "step 3850, loss: 1.680133581161499\n",
      "step 3860, loss: 1.5760588645935059\n",
      "step 3870, loss: 1.7293628454208374\n",
      "step 3880, loss: 1.697558879852295\n",
      "step 3890, loss: 1.7581837177276611\n",
      "step 3900, loss: 1.6634613275527954\n",
      "step 3910, loss: 1.7484461069107056\n",
      "step 3920, loss: 1.758063793182373\n",
      "step 3930, loss: 1.8136661052703857\n",
      "step 3940, loss: 1.6500980854034424\n",
      "step 3950, loss: 1.5822834968566895\n",
      "step 3960, loss: 1.7308273315429688\n",
      "step 3970, loss: 1.7411324977874756\n",
      "step 3980, loss: 1.7485491037368774\n",
      "step 3990, loss: 1.6580544710159302\n",
      "step 4000, loss: 1.8492286205291748\n",
      "step 4010, loss: 1.6244022846221924\n",
      "step 4020, loss: 1.697990894317627\n",
      "step 4030, loss: 1.65213942527771\n",
      "step 4040, loss: 1.604672908782959\n",
      "step 4050, loss: 1.7243685722351074\n",
      "step 4060, loss: 1.742002248764038\n",
      "step 4070, loss: 1.6768746376037598\n",
      "step 4080, loss: 1.7667847871780396\n",
      "step 4090, loss: 1.7745850086212158\n",
      "step 4100, loss: 1.7617459297180176\n",
      "step 4110, loss: 1.7461835145950317\n",
      "step 4120, loss: 1.6392747163772583\n",
      "step 4130, loss: 1.6817455291748047\n",
      "step 4140, loss: 1.6402101516723633\n",
      "step 4150, loss: 1.5016800165176392\n",
      "step 4160, loss: 1.7770328521728516\n",
      "step 4170, loss: 1.7309236526489258\n",
      "step 4180, loss: 1.6531821489334106\n",
      "step 4190, loss: 1.8224900960922241\n",
      "step 4200, loss: 1.6988403797149658\n",
      "step 4210, loss: 1.6273155212402344\n",
      "step 4220, loss: 1.6396074295043945\n",
      "step 4230, loss: 1.6773992776870728\n",
      "step 4240, loss: 1.6479437351226807\n",
      "step 4250, loss: 1.5941352844238281\n",
      "step 4260, loss: 1.6312209367752075\n",
      "step 4270, loss: 1.727103590965271\n",
      "step 4280, loss: 1.5467835664749146\n",
      "step 4290, loss: 1.6317626237869263\n",
      "step 4300, loss: 1.6857831478118896\n",
      "step 4310, loss: 1.776840329170227\n",
      "step 4320, loss: 1.662166953086853\n",
      "step 4330, loss: 1.5645112991333008\n",
      "step 4340, loss: 1.7549564838409424\n",
      "step 4350, loss: 1.7123875617980957\n",
      "step 4360, loss: 1.6496925354003906\n",
      "step 4370, loss: 1.521888017654419\n",
      "step 4380, loss: 1.7224518060684204\n",
      "step 4390, loss: 1.6703603267669678\n",
      "step 4400, loss: 1.7652225494384766\n",
      "step 4410, loss: 1.7571488618850708\n",
      "step 4420, loss: 1.719028115272522\n",
      "step 4430, loss: 1.5839760303497314\n",
      "step 4440, loss: 1.7059969902038574\n",
      "step 4450, loss: 1.6963996887207031\n",
      "step 4460, loss: 1.6610572338104248\n",
      "step 4470, loss: 1.7184436321258545\n",
      "step 4480, loss: 1.5793743133544922\n",
      "step 4490, loss: 1.6889373064041138\n",
      "step 4500, loss: 1.6245861053466797\n",
      "step 4510, loss: 1.7705026865005493\n",
      "step 4520, loss: 1.6233456134796143\n",
      "step 4530, loss: 1.5730595588684082\n",
      "step 4540, loss: 1.752401351928711\n",
      "step 4550, loss: 1.594522476196289\n",
      "step 4560, loss: 1.5860743522644043\n",
      "step 4570, loss: 1.664833664894104\n",
      "step 4580, loss: 1.6965030431747437\n",
      "step 4590, loss: 1.7424918413162231\n",
      "step 4600, loss: 1.5273829698562622\n",
      "step 4610, loss: 1.7563745975494385\n",
      "step 4620, loss: 1.757554292678833\n",
      "step 4630, loss: 1.5857114791870117\n",
      "step 4640, loss: 1.7074164152145386\n",
      "step 4650, loss: 1.7529160976409912\n",
      "step 4660, loss: 1.88494074344635\n",
      "step 4670, loss: 1.6479744911193848\n",
      "step 4680, loss: 1.607566475868225\n",
      "step 4690, loss: 1.7318344116210938\n",
      "step 4700, loss: 1.6715266704559326\n",
      "step 4710, loss: 1.61940336227417\n",
      "step 4720, loss: 1.702787160873413\n",
      "step 4730, loss: 1.7653604745864868\n",
      "step 4740, loss: 1.7422279119491577\n",
      "step 4750, loss: 1.6154588460922241\n",
      "step 4760, loss: 1.681746006011963\n",
      "step 4770, loss: 1.590350866317749\n",
      "step 4780, loss: 1.63203763961792\n",
      "step 4790, loss: 1.6868901252746582\n",
      "step 4800, loss: 1.7204198837280273\n",
      "step 4810, loss: 1.753702163696289\n",
      "step 4820, loss: 1.562040090560913\n",
      "step 4830, loss: 1.6149089336395264\n",
      "step 4840, loss: 1.6392261981964111\n",
      "step 4850, loss: 1.6601074934005737\n",
      "step 4860, loss: 1.4581602811813354\n",
      "step 4870, loss: 1.6645151376724243\n",
      "step 4880, loss: 1.4811697006225586\n",
      "step 4890, loss: 1.6970577239990234\n",
      "step 4900, loss: 1.6629559993743896\n",
      "step 4910, loss: 1.6679134368896484\n",
      "step 4920, loss: 1.7572457790374756\n",
      "step 4930, loss: 1.5740221738815308\n",
      "step 4940, loss: 1.5966386795043945\n",
      "step 4950, loss: 1.6110866069793701\n",
      "step 4960, loss: 1.6295522451400757\n",
      "step 4970, loss: 1.471733570098877\n",
      "step 4980, loss: 1.5899615287780762\n",
      "step 4990, loss: 1.5852160453796387\n",
      "step 5000, loss: 1.5946353673934937\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "optimiser = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "model.to(device)\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'parameters')\n",
    "losses = []\n",
    "for steps in range(1,steps+1):\n",
    "\n",
    "    x_batch, y_batch = get_batch('train')\n",
    "\n",
    "    logits, loss = model(x_batch, y_batch)\n",
    "    optimiser.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "    losses += [loss.item()]\n",
    "    if steps % 10 == 0:\n",
    "        print(f'step {steps}, loss: {loss.item()}')\n",
    "\n",
    "    \n",
    "# plot losses\n",
    "plt.plot(losses)\n",
    "plt.xlabel('step')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "And blood thou tulo boot frience mysen:\n",
      "I all will warwiccer: you?\n",
      "\n",
      "FRIAR SSIO:\n",
      "Ha! Dight it so seul: that aall,\n",
      "He weath in Anded croft sing,\n",
      "What tilt, Gachusion'd drave to mast some all old\n",
      "Unclage thought and me\n",
      "Thank I hate ray's, or Glace: and my leasEd.\n",
      "\n",
      "SLOrs MIO:\n",
      "He with be love, show the some.\n",
      "\n",
      "EDWARWICK EDWARD:\n",
      "Come, to for cannot proping in time in\n",
      "the will fientlys to lip, thousant face with I would tide?\n",
      "\n",
      "EETER:\n",
      "To embing mattle sight, shall to cleare your fapless, and is I have capt.\n",
      "\n",
      "AUOHN PERbost:\n",
      "Doo benvangger off:\n",
      "Poon, beitgon off good, at dauther.\n",
      "\n",
      "CLICIO:\n",
      "Whow with man'st be stais.\n",
      "\n",
      "LEORTESS OF AUMERY VOLINGBERK:\n",
      "Should Tafefers, sheae footty daylf conferscatled sout dancly,\n",
      "Timent kiln of my pooun you, good cuse\n",
      "out good away but cavake, are thought.\n",
      "\n",
      "DUKE VINCESTIV:\n",
      "My lord not into brunts! for tedom.\n",
      "\n",
      "AUTOLYt:\n",
      "Again inquestle, peec ant as but that king of my lor\n"
     ]
    }
   ],
   "source": [
    "model.to('mps')\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device='mps')\n",
    "print(decode(model.generate(context, max_new_tokens=900)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
